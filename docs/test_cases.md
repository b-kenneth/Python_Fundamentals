# Manual Test Plan and Results

This table records the main system behaviors required by the project, including expected/actual results.

---

### Test Case Table

| Test # | What to Test                                | Input/Action                                                  | Expected Result                                                                                              | Actual Result                                                                                                       | Pass/Fail | Screenshot to Include                                     |
|--------|---------------------------------------------|---------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-----------|-----------------------------------------------------------|
| 1      | CSV file generation                        | Wait 15 seconds after pipeline startup                        | 3 new CSV files (`events_YYYYMMDD_HHMMSS.csv`) in `data/events/` with correct columns/rows                   | 3 CSV files seen in `data/events/`, each with all required headers and 10–50 rows                                   | Pass      | ![Test1 Screenshot](../docs/screenshots/Screenshot%201.png)      |
| 2      | CSV schema/fields                          | Open a recent CSV file in a text editor or spreadsheet        | Columns: user_id, session_id, actions, product_id, price, event_time, user_agent; plausible fake data         | All columns present, data formats correct, e.g., ISO timestamps, price as float                                     | Pass      | Screenshot of CSV opened in Excel/VSCode/Notepad         |
| 3      | Spark detects and processes new files      | Open Spark UI after pipeline is running                       | "Input Rate" and "Processed Files" increase every batch; new files seen in "Batch Details"                   | Spark UI shows batch processing every ~5 sec; batch stats update as new CSVs arrive                                 | Pass      | Browser screenshot of Spark UI (http://localhost:4040)   |
| 4      | Data written to PostgreSQL                 | After a batch, refresh pgAdmin events table                   | All valid CSV rows appear as new records within 5–10 sec after CSV creation                                   | Rows show up in pgAdmin's SQL query window, event_time matches CSV, event_id auto-increments                       | Pass      | Screenshot of pgAdmin with events table data             |
| 5      | Table schema matches project requirements  | Inspect events table properties in pgAdmin                    | Columns/types: event_id (serial), user_id (int), session_id (text), actions, product_id, price, event_time, user_agent | All columns/types correct, event_id increments as expected, no missing/extra fields                                 | Pass      | Screenshot of pgAdmin table structure/schema tab         |
| 6      | Performance: file-to-DB latency            | Note CSV creation time, then timestamp of row in DB           | Per-file latency < 10 sec (CSV → DB insert time)                                                              | For CSV at 18:54:53, rows appear in DB by 18:54:53:4455 (avg. ~4455 msec)                                                   | Pass      | Timestamps in CSV, Spark log, and pgAdmin                |
| 7      | Performance: throughput                    | ~60 events produced over 30 seconds, check count in DB        | At least 60 events inserted in DB over 30 sec, as seen from pgAdmin count or query                             | 62 events inserted in 30 sec, matching generator output                                                            | Pass      | Screenshot of pgAdmin with count query/results           |
| 8      | System restart/persistence                 | Stop all services with `docker-compose down`, then restart    | Previously written events remain in DB; new data resumes; pipeline works as before                            | After restart, all previous rows still in events table, pipeline resumes normal ingestion                          | Pass      | Screenshot of DB before and after restart (same row count)|


